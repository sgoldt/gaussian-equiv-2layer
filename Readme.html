<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<title>Readme.html</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>

</head>

<body>

<h1 id="codeforthegaussianequivalenceofgenerativemodelsforlearningwithtwo-layerneuralnetworks">Code for &#8220;The Gaussian equivalence of generative models for learning with two-layer neural networks&#8221;</h1>

<p>Here we provide the code used to run all the experiments of our recent paper on
the Gaussian Equivalence of generative models. There are several parts to this
package: (for step-by-step explanations, see below)</p>

<table>
<colgroup>
<col />
<col />
</colgroup>

<thead>
<tr>
	<th> File       </th>
	<th> Description                                     </th>
</tr>
</thead>

<tbody>
<tr>
	<td> <code>covariance_generator.py</code> </td>
	<td> Estimates the covariances of a generative neural network,<br>implemented using the <a href="http://pytorch.org/">pyTorch</a> library,          </td>
</tr>
<tr>
	<td> <code>dcgan.py</code>    </td>
	<td> An implementation of a deep convolutional GAN of Radford et<br>al. [1], provided by <a href="https://github.com/pytorch/examples/tree/master/dcgan">pyTorch examples</a> </td>
</tr>
<tr>
	<td> <code>deepgen_ode.cpp</code>   </td>
	<td> An integrator for the dynamical equations derived in the paper,<br>with its <code>Makefile</code>                  </td>
</tr>
<tr>
	<td> <code>deepgen_online.py</code>  </td>
	<td> Trains two-layer neural networks when inputs are drawn from a generator                      </td>
</tr>
<tr>
	<td> <code>generators.py</code>   </td>
	<td> Provides fully-connected, deep generative neural networks                          </td>
</tr>
<tr>
	<td> <code>libscmpp.h</code>    </td>
	<td> C++ utility functions                                   </td>
</tr>
<tr>
	<td> <code>models</code>     </td>
	<td> random and pre-trained weights used for the experiments with the DCGAN,<br>as well as the corresponding covariance matrices         </td>
</tr>
<tr>
	<td> <code>twolayer.py</code>    </td>
	<td> Python utility functions                                  </td>
</tr>
</tbody>
</table>

<h1 id="compilationoftheccode">Compilation of the C++ code</h1>

<p>To compile locally, simply type</p>

<pre><code>make deepgen_ode.exe
</code></pre>

<p>This assumes that you have installed the <a href="http://arma.sourceforge.net">Armadillo
library</a> on your machine.</p>

<h1 id="step-by-stepinstructionstoreproducetheexperimentsofsec.3">Step-by-step instructions to reproduce the experiments of Sec. 3</h1>

<p>There are two parts to reproducing the experiments of Section 3: training a
neural network on inputs drawn from a particular generator, and integrating the
dynamical equations that predict the evolution of the test error and of the
order parameters.</p>

<h2 id="traininganeuralnetwork">Training a neural network</h2>

<p>To train a two-layer model on input drawn from a generative model
<code>dcgan_rand</code>, type</p>

<pre><code>./deepgen_online.py -M 2 -K 2 --lr 0.2 --scenario dcgan_rand
</code></pre>

<p>Other options for scenario are <code>rand</code>, which is the random one-layer
generator corresponding to Theorem 1, <code>dcgan_rand</code>, which is the DCGAN with
random weights, and <code>dcgan_cifar10</code> which is the DCGAN with pre-trained
weights, provided by <a href="https://github.com/csinva/gan-vae-pretrained-pytorch">Chandan
Singh</a>. This command will train an actual network with K hidden nodes, while the teacher
has M hidden nodes, and the learning rate is 0.2. For a full overview over the
parameters, run</p>

<pre><code>./deepgen_online.py --help.
</code></pre>

<p>A run of deepgen-online.py will generate various output files, with all have the
same file name root, in this case: deepgen_online_dcgan_rand_D100_N3072_hmm_erf_M2_K2_lr0.2_i1_s0.</p>

<h2 id="integratingthedynamicalequations">Integrating the dynamical equations</h2>

<p>The second step is integrating the ODEs. To this end, you will have to first
compile the ODE integrator, which is written in C++ and uses the <a href="http://arma.sourceforge.net">Armadillo
library</a>. Once this programme is available, you can
simply run</p>

<pre><code>./deepgen_ode.exe -N 3072 -M 2 -K 2 --lr 0.2 --prefix deepgen_online_dcgan_rand_D100_N3072_hmm_erf_M2_K2_lr0.2_i1_s0
</code></pre>

<p>Make sure that the parameters M, K, and lr match the values you provided for the
simulation. This will generate an additional file, in this case called
deepgen_online_dcgan_rand_D100_N3072_hmm_erf_M2_K2_lr0.2_i1_s0_ode.dat, that
contains the output of the ODE integrator. The columns of the output file are
explained in the comment lines in the header of the output file.</p>

<h1 id="references">References</h1>

<p>[1] Radford, Alec, Luke Metz, and Soumith Chintala. &#8220;Unsupervised representation
learning with deep convolutional generative adversarial networks.&#8221; ICLR 2016, <a href="http://arxiv.org/abs/1511.06434">arXiv:1511.06434</a></p>

</body>
</html>
